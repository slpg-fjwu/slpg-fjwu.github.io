---
---
@inproceedings{firdous2023biomedical,
  title={Biomedical Parallel Sentence Retrieval using Large Language Models},
  author={Firdous, Sheema and Rauf, Sadaf Abdul},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={263--270},
  year={2023}
}
@inproceedings{firdous2023biomedical,
  title={Biomedical Parallel Sentence Retrieval using Large Language Models},
  author={Firdous, Sheema and Rauf, Sadaf Abdul},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={263--270},
  year={2023},
  preview={data-processing.gif}

}

@inproceedings{haq2022context,
  title={Context-Aware Neural Machine Translation using Selected Context},
  author={Haq, Sami Ul and Rauf, Sadaf Abdul and Shaukat, Arslan and Arif, Muhammad Hassan},
  booktitle={2022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST)},
  pages={349--352},
  year={2022},
  organization={IEEE},
  selected={true},
  preview={selected-ctx-nlp.gif}
  }




  @inproceedings{naz-etal-2021-fjwu, 
    title = "{FJWU} Participation for the {WMT}21 Biomedical Translation Task",
    author = "Naz, Sumbal  and
      Abdul Rauf, Sadaf  and
      Haq, Sami Ul",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.86",
    pages = "857--862",
     preview={doc.gif},
    abstract = "In this paper we present the FJWU{'}s system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, Spanish and French) with English as target language. Our NMT systems based on Transformer architecture, were trained on combination of in-domain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.",
}
@inproceedings{naz-etal-2020-fjwu,
    bibtex_show={true}, 
    title = "{FJWU} participation for the {WMT}20 Biomedical Translation Task",
    author = "Naz, Sumbal  and
      Abdul Rauf, Sadaf  and
      Hira, Noor-e-  and
      Ul Haq, Sami",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.92",
    pages = "849--856",
    abstract = "This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using in-domain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.",
    selected={true},
    preview={nlp-art1.jpg}
}
@inproceedings{ul-haq-etal-2020-document,
    title = "Document Level {NMT} of Low-Resource Languages with Backtranslation",
    author = "Ul Haq, Sami  and
      Abdul Rauf, Sadaf  and
      Shaukat, Arsalan  and
      Saeed, Abdullah",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.53",
    pages = "442--446",
     preview={doc-level-NMT.gif},

    abstract = "This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathiâˆ’Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.",
}
@inproceedings{naz-etal-2021-fjwu,
    title = "{FJWU} Participation for the {WMT}21 Biomedical Translation Task",
    author = "Naz, Sumbal  and
      Abdul Rauf, Sadaf  and
      Haq, Sami Ul",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.86",
    pages = "857--862",
    abstract = "In this paper we present the FJWU{'}s system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, Spanish and French) with English as target language. Our NMT systems based on Transformer architecture, were trained on combination of in-domain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.",
    selected={true},
    preview={nlp-art3.jpg}
}
@inproceedings{ul-haq-etal-2020-improving,
    title = "Improving Document-Level Neural Machine Translation with Domain Adaptation",
    author = "Ul Haq, Sami  and
      Abdul Rauf, Sadaf  and
      Shoukat, Arslan  and
      Hira, Noor-e-",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.27",
    doi = "10.18653/v1/2020.ngt-1.27",
     preview={doc-NMT.gif},

    pages = "225--231",
    abstract = "Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture contextual information from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed systems by exploiting limited in-domain data. This paper presents FJWU{'}s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.",
}
